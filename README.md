# ğŸ¥ Medical Multimodal Report Generation

ì˜ë£Œ ì˜ìƒ(X-ray, CT, MRI)ì„ ì…ë ¥ë°›ì•„ ìë™ìœ¼ë¡œ íŒë… ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ AI ì‹œìŠ¤í…œ

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

### ëª©í‘œ
- ì˜ë£Œ ì˜ìƒì—ì„œ ìë™ìœ¼ë¡œ íŒë… ë¦¬í¬íŠ¸ ìƒì„±
- Vision Encoderì™€ Language Decoderë¥¼ ê²°í•©í•œ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ êµ¬í˜„
- ì˜ë£Œ ë„ë©”ì¸ì— íŠ¹í™”ëœ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ì‹œìŠ¤í…œ ê°œë°œ

### ì£¼ìš” ê¸°ëŠ¥
- ì˜ë£Œ ì˜ìƒ ìë™ ë¶„ì„
- ìì—°ì–´ ë¦¬í¬íŠ¸ ìƒì„±
- Attention visualizationìœ¼ë¡œ í•´ì„ ê°€ëŠ¥ì„± ì œê³µ
- ì›¹ ê¸°ë°˜ ì¸í„°í˜ì´ìŠ¤ (Gradio/Streamlit)

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
Medical Image Input
        â†“
  Vision Encoder (ResNet/ViT)
        â†“
  Image Features
        â†“
  Cross-Attention Layer â† Text Embeddings
        â†“
  Language Decoder (GPT-2/BART)
        â†“
  Medical Report Output
```

## ğŸ—‚ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
project1_medical_multimodal/
â”œâ”€â”€ data/                      # ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ raw/                   # ì›ë³¸ ë°ì´í„°
â”‚   â”œâ”€â”€ processed/             # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”‚   â””â”€â”€ external/              # ì™¸ë¶€ ë°ì´í„°
â”œâ”€â”€ models/                    # ëª¨ë¸ ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ checkpoints/           # í•™ìŠµ ì¤‘ ì²´í¬í¬ì¸íŠ¸
â”‚   â””â”€â”€ final/                 # ìµœì¢… ëª¨ë¸
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”‚   â”œâ”€â”€ exploration/           # ë°ì´í„° íƒìƒ‰
â”‚   â””â”€â”€ experiments/           # ì‹¤í—˜ ë…¸íŠ¸ë¶
â”œâ”€â”€ src/                       # ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ data/                  # ë°ì´í„° ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ dataset.py         # Dataset í´ë˜ìŠ¤
â”‚   â”‚   â””â”€â”€ preprocessing.py   # ì „ì²˜ë¦¬ í•¨ìˆ˜
â”‚   â”œâ”€â”€ models/                # ëª¨ë¸ ì •ì˜
â”‚   â”‚   â”œâ”€â”€ vision_encoder.py  # Vision Encoder
â”‚   â”‚   â”œâ”€â”€ language_decoder.py # Language Decoder
â”‚   â”‚   â””â”€â”€ multimodal_model.py # í†µí•© ëª¨ë¸
â”‚   â”œâ”€â”€ training/              # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”‚   â”œâ”€â”€ train.py           # í•™ìŠµ ë©”ì¸
â”‚   â”‚   â””â”€â”€ evaluate.py        # í‰ê°€
â”‚   â”œâ”€â”€ inference/             # ì¶”ë¡ 
â”‚   â”‚   â””â”€â”€ predict.py         # ì˜ˆì¸¡
â”‚   â””â”€â”€ utils/                 # ìœ í‹¸ë¦¬í‹°
â”‚       â”œâ”€â”€ metrics.py         # í‰ê°€ ì§€í‘œ
â”‚       â””â”€â”€ visualization.py   # ì‹œê°í™”
â”œâ”€â”€ tests/                     # í…ŒìŠ¤íŠ¸ ì½”ë“œ
â”œâ”€â”€ outputs/                   # ì¶œë ¥ë¬¼
â”‚   â”œâ”€â”€ reports/               # ìƒì„±ëœ ë¦¬í¬íŠ¸
â”‚   â””â”€â”€ visualizations/        # ì‹œê°í™” ê²°ê³¼
â”œâ”€â”€ configs/                   # ì„¤ì • íŒŒì¼
â”œâ”€â”€ docs/                      # ë¬¸ì„œ
â”œâ”€â”€ requirements.txt           # íŒ¨í‚¤ì§€ ì˜ì¡´ì„±
â”œâ”€â”€ setup.sh                   # ì´ˆê¸° ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ .gitignore                 # Git ì œì™¸ íŒŒì¼
â””â”€â”€ README.md                  # í”„ë¡œì íŠ¸ ì„¤ëª…
```

## ğŸš€ ì‹œì‘í•˜ê¸°

### 1. í™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv

# ê°€ìƒí™˜ê²½ í™œì„±í™” (Windows)
venv\Scripts\activate

# ê°€ìƒí™˜ê²½ í™œì„±í™” (Linux/Mac)
source venv/bin/activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

### 2. ë°ì´í„° ì¤€ë¹„

```bash
# MIMIC-CXR ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (PhysioNet ê³„ì • í•„ìš”)
# https://physionet.org/content/mimic-cxr/2.0.0/

# ë°ì´í„° ì „ì²˜ë¦¬
python src/data/preprocessing.py --input data/raw --output data/processed
```

### 3. ëª¨ë¸ í•™ìŠµ

```bash
# ê¸°ë³¸ í•™ìŠµ
python src/training/train.py --config configs/base_config.yaml

# GPU ì‚¬ìš©
python src/training/train.py --config configs/base_config.yaml --gpu 0

# ë©€í‹° GPU ì‚¬ìš©
python src/training/train.py --config configs/base_config.yaml --gpus 0,1,2,3
```

### 4. ëª¨ë¸ í‰ê°€

```bash
python src/training/evaluate.py --model models/final/best_model.pth --data data/processed/test
```

### 5. ì¶”ë¡  ë° ë°ëª¨

```bash
# ë‹¨ì¼ ì´ë¯¸ì§€ ì˜ˆì¸¡
python src/inference/predict.py --image path/to/image.jpg --model models/final/best_model.pth

# Gradio ì›¹ ë°ëª¨ ì‹¤í–‰
python app.py
```

## ğŸ“Š ë°ì´í„°ì…‹

### MIMIC-CXR
- **ì„¤ëª…**: í‰ë¶€ X-ray ì´ë¯¸ì§€ì™€ íŒë… ë¦¬í¬íŠ¸
- **í¬ê¸°**: ~377,110 ì´ë¯¸ì§€, ~227,835 ë¦¬í¬íŠ¸
- **ë§í¬**: https://physionet.org/content/mimic-cxr/

### IU X-Ray
- **ì„¤ëª…**: í‰ë¶€ X-rayì™€ ì§„ë‹¨ ë¦¬í¬íŠ¸
- **í¬ê¸°**: 7,470 ì´ë¯¸ì§€, 3,955 ë¦¬í¬íŠ¸
- **ë§í¬**: https://openi.nlm.nih.gov/

## ğŸ¯ Week 1 ëª©í‘œ

### Day 1-2: ë°ì´í„° ì¤€ë¹„
- [ ] MIMIC-CXR ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° íƒìƒ‰
- [ ] ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- [ ] Train/Val/Test ë¶„í• 

### Day 3-4: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸
- [ ] Vision Encoder êµ¬í˜„ (ResNet/ViT)
- [ ] Language Decoder êµ¬í˜„ (GPT-2/BART)
- [ ] ê¸°ë³¸ í•™ìŠµ ë£¨í”„ ì‘ì„±

### Day 5-7: ë©€í‹°ëª¨ë‹¬ ìœµí•©
- [ ] Cross-attention ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„
- [ ] ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœµí•© ë ˆì´ì–´
- [ ] ì´ˆê¸° í•™ìŠµ ë° ê²€ì¦

## ğŸ“ˆ í‰ê°€ ì§€í‘œ

- **BLEU**: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„
- **ROUGE**: ìš”ì•½ í’ˆì§ˆ
- **METEOR**: ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„
- **CIDEr**: Consensus-based í‰ê°€
- **Clinical Accuracy**: RadGraph ê¸°ë°˜

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

- **Deep Learning**: PyTorch, PyTorch Lightning
- **Vision**: torchvision, OpenCV
- **NLP**: Transformers, tokenizers
- **Medical**: pydicom, SimpleITK
- **Visualization**: matplotlib, seaborn, plotly
- **Deployment**: Gradio, Streamlit

## ğŸ“ ì°¸ê³  ë…¼ë¬¸

1. "Show and Tell: A Neural Image Caption Generator" (Vinyals et al., 2015)
2. "Attention is All You Need" (Vaswani et al., 2017)
3. "CLIP: Connecting Text and Images" (Radford et al., 2021)
4. "CheXbert: Combining Automatic Labelers and Expert Annotations" (Smit et al., 2020)

## ğŸ¤ ê¸°ì—¬

ì´ í”„ë¡œì íŠ¸ëŠ” ê°œì¸ í¬íŠ¸í´ë¦¬ì˜¤ ëª©ì ìœ¼ë¡œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License

## ğŸ‘¤ ì‘ì„±ì

- **ì´ë¦„**: [Your Name]
- **ì´ë©”ì¼**: [Your Email]
- **GitHub**: [Your GitHub]
- **LinkedIn**: [Your LinkedIn]

## ğŸ™ ê°ì‚¬ì˜ ê¸€

- MIMIC-CXR ë°ì´í„°ì…‹ ì œê³µ: PhysioNet
- Hugging Face Transformers íŒ€
- PyTorch ì»¤ë®¤ë‹ˆí‹°

---

**í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©**: Week 1 ì¤€ë¹„ ì¤‘ ğŸš§

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2024-10-23
